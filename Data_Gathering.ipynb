{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb4e990-f738-41d4-aaab-e13fdda8a18a",
   "metadata": {},
   "source": [
    "## 📂 `Working with CSV Files`  \n",
    "\n",
    "**CSV (Comma-Separated Values)** files are one of the most common formats used for storing and exchanging tabular data.  \n",
    "Each line in a CSV file represents a data record, and each record consists of fields separated by commas.  \n",
    "They are simple, lightweight, and compatible with most data analysis tools like **Pandas, Excel, Google Sheets, and SQL databases**.\n",
    "\n",
    "### 🔍 How We Work with CSV Files\n",
    "We usually import and manipulate CSV files using libraries like **Pandas** in Python.  \n",
    "This allows us to clean, filter, and prepare data for analysis or model training.  \n",
    "\n",
    "### 📘 Key Points\n",
    "- CSV is widely used for **structured datasets**.  \n",
    "- Easy to read and write using Python’s `pandas.read_csv()` and `to_csv()`.  \n",
    "- Common in **data collection pipelines** and **data preprocessing**.  \n",
    "- Data should always be checked for **missing values**, **encoding issues**, and **duplicates**.  \n",
    "- CSV remains the **most preferred format in 2025** for initial data exchange across industries.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 Data Gathering\n",
    "\n",
    "Below is a visual representation of different data-gathering methods in Machine Learning:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[📊 Data Gathering] --> B[📁 CSV Files]\n",
    "    A --> C[🧾 JSON / SQL Databases]\n",
    "    A --> D[🌐 Fetch API]\n",
    "    A --> E[🕸️ Web Scraping]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6270c23-e061-4aa9-a0e6-1f3ff174b294",
   "metadata": {},
   "source": [
    "## 🧾 `Working with JSON / SQL`\n",
    "\n",
    "### 📘 JSON (JavaScript Object Notation)\n",
    "\n",
    "**JSON** is a lightweight data format often used for **data interchange between web applications and servers**.  \n",
    "It represents data as key–value pairs and supports nested structures, making it ideal for **API responses** and **configuration files**.\n",
    "\n",
    "#### 🔑 Important Points\n",
    "- Easy to parse in Python using the built-in `json` module or `pandas.read_json()`.\n",
    "- Common in **web APIs** and **NoSQL databases** like MongoDB.\n",
    "- Human-readable and supports **hierarchical data structures**.\n",
    "- JSON is widely used in **real-time data pipelines** and **AI-driven web applications** (2025 trend).\n",
    "\n",
    "---\n",
    "\n",
    "### 🗄️ SQL (Structured Query Language)\n",
    "\n",
    "**SQL** databases store data in **tables with rows and columns**, following a fixed schema.  \n",
    "They are used for managing **large-scale, structured data** efficiently and are still dominant in enterprise systems.\n",
    "\n",
    "#### 🔑 Important Points\n",
    "- Used with relational databases like **MySQL, PostgreSQL, and SQLite**.\n",
    "- Data can be fetched directly using SQL queries or Python libraries like `sqlite3` and `SQLAlchemy`.\n",
    "- Ideal for **large, relational, and transactional datasets**.\n",
    "- SQL remains the **backbone of data storage** in most production-grade ML pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d49ce6-4e09-40c8-95a4-8d7b9b8dc696",
   "metadata": {},
   "source": [
    "## 🌐 `How to Fetch Data from an API`\n",
    "\n",
    "### 🔹 What is an API?\n",
    "\n",
    "An **API (Application Programming Interface)** is a communication bridge that allows two software systems to interact and exchange data.  \n",
    "It defines a set of rules and endpoints that let applications **request** or **send** information securely and efficiently.\n",
    "\n",
    "In simple terms — APIs enable your program to talk to other systems or services (like weather apps, financial data sources, or ML model servers).\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ How APIs Work\n",
    "\n",
    "1. A **client** (your program) sends a request to a specific API endpoint (usually a URL).  \n",
    "2. The **server** processes that request and returns a **response**, often in **JSON** format.  \n",
    "3. The client then parses and uses this data — for example, converting it into a **DataFrame** for analysis.\n",
    "\n",
    "APIs are essential in:\n",
    "- **Software Engineering:** For integrating external services (e.g., payment gateways, maps, chatbots).  \n",
    "- **Machine Learning:** For fetching **real-time data**, updating models, or connecting with data pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### 📥 Fetching Data from an API\n",
    "\n",
    "To fetch data from an API:\n",
    "1. Identify the **API endpoint (URL)** you want to use.  \n",
    "2. Send an **HTTP GET request** to that endpoint.  \n",
    "3. The API will return data (usually in JSON format).  \n",
    "4. Convert that JSON response into a **pandas DataFrame** for analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Example Workflow (Conceptually)\n",
    "\n",
    "```text\n",
    "API Endpoint (URL)  →  Send Request  →  Receive JSON Response  \n",
    "       ↓  \n",
    "Parse JSON Data  →  Convert to Pandas DataFrame  →  Use for ML Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8bb61-3284-44ba-b741-2b7a6fd73bba",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Client] --> B[API Endpoint]\n",
    "  B --> C[JSON Response]\n",
    "  C --> D[Parse JSON]\n",
    "  D --> E[Pandas DataFrame]\n",
    "  E --> F[ML Model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a93a1a-cdaf-47a0-84db-6511c3ad90f4",
   "metadata": {},
   "source": [
    "## 🕸️ `Fetching Data Using Web Scraping`\n",
    "\n",
    "**Web Scraping** is the process of automatically extracting useful information from websites.  \n",
    "When APIs or downloadable datasets are not available, web scraping allows us to collect **custom, real-world data** directly from web pages.\n",
    "\n",
    "### 🔹 How It Works\n",
    "1. Send an HTTP request to a web page (using tools like `requests` or `urllib`).  \n",
    "2. The server responds with the HTML content of the page.  \n",
    "3. Parse and extract specific elements (e.g., titles, prices, reviews) using libraries such as **BeautifulSoup**, **Scrapy**, or **Selenium**.  \n",
    "4. Store the cleaned data in structured formats like **CSV**, **JSON**, or **databases** for analysis or ML tasks.\n",
    "\n",
    "### ⚙️ Common Tools\n",
    "- **BeautifulSoup** – for HTML parsing and tag-based data extraction.  \n",
    "- **Requests** – to send GET or POST requests to websites.  \n",
    "- **Selenium** – for scraping dynamic (JavaScript-rendered) websites.  \n",
    "- **Scrapy** – for large-scale or automated web crawling projects.\n",
    "\n",
    "### ⚠️ Important Considerations\n",
    "- Always check a website’s **robots.txt** before scraping.  \n",
    "- Respect **rate limits** and website terms of service.  \n",
    "- Avoid overloading servers with too many requests.  \n",
    "- Use scraping ethically — for learning or research purposes.\n",
    "\n",
    "### 💡 Tip\n",
    "Web scraping helps when you need **unique datasets** — for example, collecting real-time product prices, news headlines, or weather data — making it a valuable data gathering technique in ML pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3b1c0-593c-4413-bf77-c3a99be59289",
   "metadata": {},
   "source": [
    "# 🕸️ Fetching Data Using Web Scraping\n",
    "\n",
    "Web scraping is a technique used to automatically extract data from websites.  \n",
    "It’s useful when the required data isn’t available via an API or downloadable dataset.  \n",
    "In Machine Learning, web scraping helps gather real-world data for model training.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Web Scraping Data Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[🌐 Website] --> B[HTTP Request (requests / selenium)]\n",
    "  B --> C[📄 HTML Response]\n",
    "  C --> D[🧩 Parse HTML (BeautifulSoup / Scrapy)]\n",
    "  D --> E[🧹 Extract & Clean Data]\n",
    "  E --> F[💾 Structured Output (CSV / JSON / Pandas DataFrame)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f1e7c-a819-46a4-a58c-2848789f6c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
