{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb4e990-f738-41d4-aaab-e13fdda8a18a",
   "metadata": {},
   "source": [
    "## ðŸ“‚ `Working with CSV Files`  \n",
    "\n",
    "**CSV (Comma-Separated Values)** files are one of the most common formats used for storing and exchanging tabular data.  \n",
    "Each line in a CSV file represents a data record, and each record consists of fields separated by commas.  \n",
    "They are simple, lightweight, and compatible with most data analysis tools like **Pandas, Excel, Google Sheets, and SQL databases**.\n",
    "\n",
    "### ðŸ” How We Work with CSV Files\n",
    "We usually import and manipulate CSV files using libraries like **Pandas** in Python.  \n",
    "This allows us to clean, filter, and prepare data for analysis or model training.  \n",
    "\n",
    "### ðŸ“˜ Key Points\n",
    "- CSV is widely used for **structured datasets**.  \n",
    "- Easy to read and write using Pythonâ€™s `pandas.read_csv()` and `to_csv()`.  \n",
    "- Common in **data collection pipelines** and **data preprocessing**.  \n",
    "- Data should always be checked for **missing values**, **encoding issues**, and **duplicates**.  \n",
    "- CSV remains the **most preferred format in 2025** for initial data exchange across industries.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§­ Data Gathering\n",
    "\n",
    "Below is a visual representation of different data-gathering methods in Machine Learning:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[ðŸ“Š Data Gathering] --> B[ðŸ“ CSV Files]\n",
    "    A --> C[ðŸ§¾ JSON / SQL Databases]\n",
    "    A --> D[ðŸŒ Fetch API]\n",
    "    A --> E[ðŸ•¸ï¸ Web Scraping]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6270c23-e061-4aa9-a0e6-1f3ff174b294",
   "metadata": {},
   "source": [
    "## ðŸ§¾ `Working with JSON / SQL`\n",
    "\n",
    "### ðŸ“˜ JSON (JavaScript Object Notation)\n",
    "\n",
    "**JSON** is a lightweight data format often used for **data interchange between web applications and servers**.  \n",
    "It represents data as keyâ€“value pairs and supports nested structures, making it ideal for **API responses** and **configuration files**.\n",
    "\n",
    "#### ðŸ”‘ Important Points\n",
    "- Easy to parse in Python using the built-in `json` module or `pandas.read_json()`.\n",
    "- Common in **web APIs** and **NoSQL databases** like MongoDB.\n",
    "- Human-readable and supports **hierarchical data structures**.\n",
    "- JSON is widely used in **real-time data pipelines** and **AI-driven web applications** (2025 trend).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ—„ï¸ SQL (Structured Query Language)\n",
    "\n",
    "**SQL** databases store data in **tables with rows and columns**, following a fixed schema.  \n",
    "They are used for managing **large-scale, structured data** efficiently and are still dominant in enterprise systems.\n",
    "\n",
    "#### ðŸ”‘ Important Points\n",
    "- Used with relational databases like **MySQL, PostgreSQL, and SQLite**.\n",
    "- Data can be fetched directly using SQL queries or Python libraries like `sqlite3` and `SQLAlchemy`.\n",
    "- Ideal for **large, relational, and transactional datasets**.\n",
    "- SQL remains the **backbone of data storage** in most production-grade ML pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d49ce6-4e09-40c8-95a4-8d7b9b8dc696",
   "metadata": {},
   "source": [
    "## ðŸŒ `How to Fetch Data from an API`\n",
    "\n",
    "### ðŸ”¹ What is an API?\n",
    "\n",
    "An **API (Application Programming Interface)** is a communication bridge that allows two software systems to interact and exchange data.  \n",
    "It defines a set of rules and endpoints that let applications **request** or **send** information securely and efficiently.\n",
    "\n",
    "In simple terms â€” APIs enable your program to talk to other systems or services (like weather apps, financial data sources, or ML model servers).\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ How APIs Work\n",
    "\n",
    "1. A **client** (your program) sends a request to a specific API endpoint (usually a URL).  \n",
    "2. The **server** processes that request and returns a **response**, often in **JSON** format.  \n",
    "3. The client then parses and uses this data â€” for example, converting it into a **DataFrame** for analysis.\n",
    "\n",
    "APIs are essential in:\n",
    "- **Software Engineering:** For integrating external services (e.g., payment gateways, maps, chatbots).  \n",
    "- **Machine Learning:** For fetching **real-time data**, updating models, or connecting with data pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¥ Fetching Data from an API\n",
    "\n",
    "To fetch data from an API:\n",
    "1. Identify the **API endpoint (URL)** you want to use.  \n",
    "2. Send an **HTTP GET request** to that endpoint.  \n",
    "3. The API will return data (usually in JSON format).  \n",
    "4. Convert that JSON response into a **pandas DataFrame** for analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Example Workflow (Conceptually)\n",
    "\n",
    "```text\n",
    "API Endpoint (URL)  â†’  Send Request  â†’  Receive JSON Response  \n",
    "       â†“  \n",
    "Parse JSON Data  â†’  Convert to Pandas DataFrame  â†’  Use for ML Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8bb61-3284-44ba-b741-2b7a6fd73bba",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Client] --> B[API Endpoint]\n",
    "  B --> C[JSON Response]\n",
    "  C --> D[Parse JSON]\n",
    "  D --> E[Pandas DataFrame]\n",
    "  E --> F[ML Model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a93a1a-cdaf-47a0-84db-6511c3ad90f4",
   "metadata": {},
   "source": [
    "## ðŸ•¸ï¸ `Fetching Data Using Web Scraping`\n",
    "\n",
    "**Web Scraping** is the process of automatically extracting useful information from websites.  \n",
    "When APIs or downloadable datasets are not available, web scraping allows us to collect **custom, real-world data** directly from web pages.\n",
    "\n",
    "### ðŸ”¹ How It Works\n",
    "1. Send an HTTP request to a web page (using tools like `requests` or `urllib`).  \n",
    "2. The server responds with the HTML content of the page.  \n",
    "3. Parse and extract specific elements (e.g., titles, prices, reviews) using libraries such as **BeautifulSoup**, **Scrapy**, or **Selenium**.  \n",
    "4. Store the cleaned data in structured formats like **CSV**, **JSON**, or **databases** for analysis or ML tasks.\n",
    "\n",
    "### âš™ï¸ Common Tools\n",
    "- **BeautifulSoup** â€“ for HTML parsing and tag-based data extraction.  \n",
    "- **Requests** â€“ to send GET or POST requests to websites.  \n",
    "- **Selenium** â€“ for scraping dynamic (JavaScript-rendered) websites.  \n",
    "- **Scrapy** â€“ for large-scale or automated web crawling projects.\n",
    "\n",
    "### âš ï¸ Important Considerations\n",
    "- Always check a websiteâ€™s **robots.txt** before scraping.  \n",
    "- Respect **rate limits** and website terms of service.  \n",
    "- Avoid overloading servers with too many requests.  \n",
    "- Use scraping ethically â€” for learning or research purposes.\n",
    "\n",
    "### ðŸ’¡ Tip\n",
    "Web scraping helps when you need **unique datasets** â€” for example, collecting real-time product prices, news headlines, or weather data â€” making it a valuable data gathering technique in ML pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3b1c0-593c-4413-bf77-c3a99be59289",
   "metadata": {},
   "source": [
    "# ðŸ•¸ï¸ Fetching Data Using Web Scraping\n",
    "\n",
    "Web scraping is a technique used to automatically extract data from websites.  \n",
    "Itâ€™s useful when the required data isnâ€™t available via an API or downloadable dataset.  \n",
    "In Machine Learning, web scraping helps gather real-world data for model training.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Web Scraping Data Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[ðŸŒ Website] --> B[HTTP Request (requests / selenium)]\n",
    "  B --> C[ðŸ“„ HTML Response]\n",
    "  C --> D[ðŸ§© Parse HTML (BeautifulSoup / Scrapy)]\n",
    "  D --> E[ðŸ§¹ Extract & Clean Data]\n",
    "  E --> F[ðŸ’¾ Structured Output (CSV / JSON / Pandas DataFrame)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f1e7c-a819-46a4-a58c-2848789f6c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
